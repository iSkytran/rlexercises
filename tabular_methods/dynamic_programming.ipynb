{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    NORTH = 0\n",
    "    SOUTH = 1\n",
    "    EAST = 2\n",
    "    WEST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, height: int, width: int):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.rewards = -1 * np.ones((height, width))\n",
    "        self.policy = 0.25 * np.ones((height, width, len(Action)))\n",
    "        self.value = np.zeros((height, width))\n",
    "\n",
    "        # Zero reward on terminal.\n",
    "        self.rewards[0, 0] = 0\n",
    "        self.rewards[-1, -1] = 0\n",
    "\n",
    "    def print_value(self):\n",
    "        print(f\"{self.value}\\n\")\n",
    "\n",
    "    def print_policy(self):\n",
    "        for i in range(self.height):\n",
    "            str = \"|\"\n",
    "            for j in range(self.width):\n",
    "                str += \"N\" if self.policy[i, j, Action.NORTH.value] != 0 else \" \"\n",
    "                str += \"S\" if self.policy[i, j, Action.SOUTH.value] != 0 else \" \"\n",
    "                str += \"E\" if self.policy[i, j, Action.EAST.value] != 0 else \" \"\n",
    "                str += \"W\" if self.policy[i, j, Action.WEST.value] != 0 else \" \"\n",
    "                \n",
    "                if (j == self.width - 1):\n",
    "                    str += \"|\"\n",
    "                    print(str)\n",
    "                else:\n",
    "                    str += \"|\"\n",
    "        print()\n",
    "\n",
    "    def __next_state(self, i: int, j: int, action: Action) -> Tuple[int, int]:\n",
    "        # Move to next state based on action. Constrain if off border.\n",
    "        if action == Action.NORTH:\n",
    "            next_h = max(0, i - 1)\n",
    "            next_j = j\n",
    "        elif action == Action.SOUTH:\n",
    "            next_h = min(self.height - 1, i + 1)\n",
    "            next_j = j\n",
    "        elif action == Action.EAST:\n",
    "            next_h = i\n",
    "            next_j = min(self.width - 1, j + 1)\n",
    "        else:\n",
    "            next_h = i\n",
    "            next_j = max(0, j - 1)\n",
    "\n",
    "        return next_h, next_j\n",
    "\n",
    "\n",
    "    def __bellman_lookahead(self, values: np.ndarray, i: int, j: int, gamma: int) -> float:\n",
    "        # Terminal state has 0 reward.\n",
    "        if (i == 0 and j == 0) or (i == self.height -1 and j == self.width - 1):\n",
    "            return 0\n",
    "\n",
    "        total = 0\n",
    "\n",
    "        # One step lookahead.\n",
    "        for action in Action:\n",
    "            next_i, next_j = self.__next_state(i, j, action)\n",
    "            total += self.policy[i, j, action.value] * (self.rewards[next_i, next_j] + gamma * values[next_i, next_j])\n",
    "\n",
    "        return total\n",
    "\n",
    "    def __policy_evaluation(self, gamma: float, theta: float) -> int:\n",
    "        counter = 0\n",
    "        # Keep updating values until they converge based on policy.\n",
    "        while True:\n",
    "            delta = 0\n",
    "            old_values = self.value.copy()\n",
    "            for i in range(self.height):\n",
    "                for j in range(self.width):\n",
    "                    value = self.value[i, j]\n",
    "                    self.value[i, j] = self.__bellman_lookahead(old_values, i, j, gamma)\n",
    "                    delta = max(delta, abs(value - self.value[i, j]))\n",
    "\n",
    "            counter += 1\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        return counter        \n",
    "\n",
    "    def __policy_improvement(self, gamma: float) -> bool:\n",
    "        # Find best actions for each state.\n",
    "        policy_stable = True\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                policy = self.policy[i, j, :].copy()\n",
    "\n",
    "                max_value = None\n",
    "                best_actions = []\n",
    "\n",
    "                for action in Action:\n",
    "                    next_i, next_j = self.__next_state(i, j, action)\n",
    "                    value = self.rewards[next_i, next_j] + gamma * self.value[next_i, next_j]\n",
    "\n",
    "                    if max_value is None:\n",
    "                        max_value = value\n",
    "                        best_actions.append(action)\n",
    "                    elif value > max_value:\n",
    "                        max_value = value\n",
    "                        best_actions = [action]\n",
    "                    elif value == max_value:\n",
    "                        best_actions.append(action)\n",
    "\n",
    "                # Divide probability over tied best actions.\n",
    "                new_prob = 1 / len(best_actions)\n",
    "                for action in Action:\n",
    "                    self.policy[i, j, action.value] = new_prob if action in best_actions else 0\n",
    "\n",
    "                if not (self.policy[i, j, :] == policy).all():\n",
    "                    policy_stable = False\n",
    "\n",
    "        return policy_stable\n",
    "\n",
    "    def policy_iteration(self, gamma: float = 1, theta: float = 0.1):\n",
    "        print(\"Starting Policy Iteration\")\n",
    "        counter = 0\n",
    "        while True:\n",
    "            counter += 1\n",
    "            eval_count = self.__policy_evaluation(gamma, theta)\n",
    "            print(f\"Policy evaluation for improvement iteration {counter} took {eval_count} iterations to converge.\")\n",
    "            if self.__policy_improvement(gamma):\n",
    "                break\n",
    "        \n",
    "        print(f\"{counter} policy improvement iterations for policy convergence.\")\n",
    "        print(\"Value Function:\")\n",
    "        self.print_value()\n",
    "        print(\"Policy:\")\n",
    "        self.print_policy()\n",
    "\n",
    "    def __optimal_bellman(self, values: np.ndarray, i: int, j: int, gamma: float) -> float:\n",
    "        # Terminal state has 0 reward.\n",
    "        if (i == 0 and j == 0) or (i == self.height -1 and j == self.width - 1):\n",
    "            return 0\n",
    "\n",
    "        max_value = None\n",
    "\n",
    "        # One step lookahead.\n",
    "        for action in Action:\n",
    "            next_i, next_j = self.__next_state(i, j, action)\n",
    "            value = self.policy[i, j, action.value] * (self.rewards[next_i, next_j] + gamma * values[next_i, next_j])\n",
    "            if max_value == None or value > max_value:\n",
    "                max_value = value\n",
    "\n",
    "        return max_value\n",
    "\n",
    "    def value_iteration(self, gamma: float = 1, theta: float = 0.000001):\n",
    "        print(\"Starting Value Iteration\")\n",
    "        counter = 0\n",
    "        # Keep updating values until they converge.\n",
    "        while True:\n",
    "            delta = 0\n",
    "            old_values = self.value.copy()\n",
    "            for i in range(self.height):\n",
    "                for j in range(self.width):\n",
    "                    value = self.value[i, j]\n",
    "                    self.value[i, j] = self.__optimal_bellman(old_values, i, j, gamma)\n",
    "                    delta = max(delta, abs(value - self.value[i, j]))\n",
    "\n",
    "            counter += 1\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        self.__policy_improvement(gamma)\n",
    "        \n",
    "        print(f\"{counter} iterations for value iteration to converge.\")\n",
    "        print(\"Value Function:\")\n",
    "        self.print_value()\n",
    "        print(\"Policy:\")\n",
    "        self.print_policy()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy Iteration\n",
      "Policy evaluation for improvement iteration 1 took 46 iterations to converge.\n",
      "Policy evaluation for improvement iteration 2 took 4 iterations to converge.\n",
      "Policy evaluation for improvement iteration 3 took 1 iterations to converge.\n",
      "3 policy improvement iterations for policy convergence.\n",
      "Value Function:\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -1.]\n",
      " [-1. -2. -1.  0.]\n",
      " [-2. -1.  0.  0.]]\n",
      "\n",
      "Policy:\n",
      "|N  W|   W|   W| S W|\n",
      "|N   |N  W|NSEW| S  |\n",
      "|N   |NSEW| SE | S  |\n",
      "|N E |  E |  E | SE |\n",
      "\n",
      "Starting Value Iteration\n",
      "3 iterations for value iteration to converge.\n",
      "Value Function:\n",
      "[[ 0.      0.     -0.25   -0.3125]\n",
      " [ 0.     -0.25   -0.3125 -0.25  ]\n",
      " [-0.25   -0.3125 -0.25    0.    ]\n",
      " [-0.3125 -0.25    0.      0.    ]]\n",
      "\n",
      "Policy:\n",
      "|N  W|   W|   W| S W|\n",
      "|N   |N  W|NSEW| S  |\n",
      "|N   |NSEW| SE | S  |\n",
      "|N E |  E |  E | SE |\n",
      "\n",
      "Starting Policy Iteration\n",
      "Policy evaluation for improvement iteration 1 took 685 iterations to converge.\n",
      "Policy evaluation for improvement iteration 2 took 14 iterations to converge.\n",
      "Policy evaluation for improvement iteration 3 took 2 iterations to converge.\n",
      "Policy evaluation for improvement iteration 4 took 2 iterations to converge.\n",
      "Policy evaluation for improvement iteration 5 took 1 iterations to converge.\n",
      "5 policy improvement iterations for policy convergence.\n",
      "Value Function:\n",
      "[[  0.   0.  -1.  -2.  -3.  -4.  -5.  -6.  -7.  -8.  -9. -10.  -9.  -8.\n",
      "   -7.  -6.  -5.]\n",
      " [  0.  -1.  -2.  -3.  -4.  -5.  -6.  -7.  -8.  -9. -10.  -9.  -8.  -7.\n",
      "   -6.  -5.  -4.]\n",
      " [ -1.  -2.  -3.  -4.  -5.  -6.  -7.  -8.  -9. -10.  -9.  -8.  -7.  -6.\n",
      "   -5.  -4.  -3.]\n",
      " [ -2.  -3.  -4.  -5.  -6.  -7.  -8.  -9. -10.  -9.  -8.  -7.  -6.  -5.\n",
      "   -4.  -3.  -2.]\n",
      " [ -3.  -4.  -5.  -6.  -7.  -8.  -9. -10.  -9.  -8.  -7.  -6.  -5.  -4.\n",
      "   -3.  -2.  -1.]\n",
      " [ -4.  -5.  -6.  -7.  -8.  -9. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.\n",
      "   -2.  -1.   0.]\n",
      " [ -5.  -6.  -7.  -8.  -9. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.\n",
      "   -1.   0.   0.]]\n",
      "\n",
      "Policy:\n",
      "|N  W|   W|   W|   W|   W|   W|   W|   W|   W|   W|   W| SEW| SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N EW|  E |  E |  E |  E |  E |  E |  E |  E |  E |  E | SE |\n",
      "\n",
      "Starting Value Iteration\n",
      "10 iterations for value iteration to converge.\n",
      "Value Function:\n",
      "[[ 0.          0.         -0.25       -0.3125     -0.328125   -0.33203125\n",
      "  -0.33300781 -0.33325195 -0.33331299 -0.33332825 -0.33333206 -0.33333302\n",
      "  -0.33333206 -0.33332825 -0.33331299 -0.33325195 -0.33300781]\n",
      " [ 0.         -0.25       -0.3125     -0.328125   -0.33203125 -0.33300781\n",
      "  -0.33325195 -0.33331299 -0.33332825 -0.33333206 -0.33333302 -0.33333206\n",
      "  -0.33332825 -0.33331299 -0.33325195 -0.33300781 -0.33203125]\n",
      " [-0.25       -0.3125     -0.328125   -0.33203125 -0.33300781 -0.33325195\n",
      "  -0.33331299 -0.33332825 -0.33333206 -0.33333302 -0.33333206 -0.33332825\n",
      "  -0.33331299 -0.33325195 -0.33300781 -0.33203125 -0.328125  ]\n",
      " [-0.3125     -0.328125   -0.33203125 -0.33300781 -0.33325195 -0.33331299\n",
      "  -0.33332825 -0.33333206 -0.33333302 -0.33333206 -0.33332825 -0.33331299\n",
      "  -0.33325195 -0.33300781 -0.33203125 -0.328125   -0.3125    ]\n",
      " [-0.328125   -0.33203125 -0.33300781 -0.33325195 -0.33331299 -0.33332825\n",
      "  -0.33333206 -0.33333302 -0.33333206 -0.33332825 -0.33331299 -0.33325195\n",
      "  -0.33300781 -0.33203125 -0.328125   -0.3125     -0.25      ]\n",
      " [-0.33203125 -0.33300781 -0.33325195 -0.33331299 -0.33332825 -0.33333206\n",
      "  -0.33333302 -0.33333206 -0.33332825 -0.33331299 -0.33325195 -0.33300781\n",
      "  -0.33203125 -0.328125   -0.3125     -0.25        0.        ]\n",
      " [-0.33300781 -0.33325195 -0.33331299 -0.33332825 -0.33333206 -0.33333302\n",
      "  -0.33333206 -0.33332825 -0.33331299 -0.33325195 -0.33300781 -0.33203125\n",
      "  -0.328125   -0.3125     -0.25        0.          0.        ]]\n",
      "\n",
      "Policy:\n",
      "|N  W|   W|   W|   W|   W|   W|   W|   W|   W|   W|   W| SEW| SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N  W|NSEW| SE | SE | SE | SE | SE | SE | SE | SE | SE | S  |\n",
      "|N   |N  W|N  W|N  W|N  W|N EW|  E |  E |  E |  E |  E |  E |  E |  E |  E |  E | SE |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_env = Environment(4, 4)\n",
    "policy_env.policy_iteration()\n",
    "\n",
    "value_env = Environment(4, 4)\n",
    "value_env.value_iteration()\n",
    "\n",
    "policy_env = Environment(7, 17)\n",
    "policy_env.policy_iteration()\n",
    "\n",
    "value_env = Environment(7, 17)\n",
    "value_env.value_iteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
